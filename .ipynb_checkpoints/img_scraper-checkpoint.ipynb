{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from threading import Thread\n",
    "from func_timeout import func_timeout, FunctionTimedOut, func_set_timeout\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "import whois\n",
    "\n",
    "#Import the CSV\n",
    "# csv = pd.read_csv(\"calls\\call-c85aac6308a09d9112b7235ab11f329c3178fd29f457deda02aec99a924e4c81.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to get the complete DF merged from all functions!\n",
    "# Ignore the 'ERRORtrying to connect to socket: closing socket' \n",
    "# The functions takes aprox 3 minutes to complete all the tasks\n",
    "\n",
    "# createDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_set_timeout(5)\n",
    "def scanMediaInfo(siteUrl):\n",
    "    #Empty list \n",
    "    allImageTags = []\n",
    "    #empty dataframe\n",
    "    dfImage = pd.DataFrame()\n",
    "\n",
    "    content = requests.get(siteUrl).content\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content,'lxml') # choose lxml parser\n",
    "    # find the tag : <img ... >\n",
    "    image_tags = soup.findAll('img')\n",
    "    #Counts all images\n",
    "    totalImages = sum([ 1 for i in soup.find_all('img')])\n",
    "    \n",
    "    for image in image_tags:\n",
    "        # CHECKS IF ALT IS EMPTY\n",
    "        if(str(image.get('alt'))==\"\"):\n",
    "            #return NONE\n",
    "            None\n",
    "        else:\n",
    "            # Else get the tag which is not empty\n",
    "            tag = str(image.get('alt'))\n",
    "        #Append the tag to the temporary list:(allImageTags)\n",
    "        temp = allImageTags.append(tag)\n",
    "\n",
    "    return totalImages, allImageTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_set_timeout(5)\n",
    "def findContentLength(siteUrl):\n",
    "    # set the url to perform the get request\n",
    "    reqs = requests.get(siteUrl)\n",
    "    # make a soup object by using beautiful soup and set the markup as html parser\n",
    "    soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "    foundWords = []\n",
    "    \n",
    "    #-------- print and count all words\n",
    "    #print(\"List of all the h1, h2, h3, h4, h5, h6 & p : \\n\")\n",
    "    for heading in soup.body.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\"]):\n",
    "        #print(heading.name + ' ' + heading.text.strip())\n",
    "        foundWords.append(heading.text.strip().split())\n",
    "        \n",
    "    #print(foundWords)\n",
    "    flatList = []\n",
    "    \n",
    "    #flatten the words \n",
    "    for sublist in foundWords:\n",
    "        for singleWord in sublist:\n",
    "            flatList.append(singleWord.lower())\n",
    "    \n",
    "    wordDensity = Counter(flatList)\n",
    "    wordDensity = wordDensity.most_common()\n",
    "\n",
    "    totalWords = str(len(flatList))\n",
    "    \n",
    "    return totalWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_set_timeout(5)\n",
    "def getMetaDetails(siteUrl):\n",
    "    #empty String (DO NOT REMOVE)\n",
    "    metaTitle = \"\"\n",
    "    try:\n",
    "        external_sites_html = urllib.request.urlopen(siteUrl)\n",
    "        websiteDomain = urlparse(siteUrl).netloc\n",
    "        soup = BeautifulSoup(external_sites_html)\n",
    "        metaTitle = soup.title.string.lstrip()\n",
    "\n",
    "        # First get the meta description tag\n",
    "        metaDescription = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})\n",
    "\n",
    "        # If description meta tag was found, then get the content attribute and save it to db entry\n",
    "        if metaDescription:\n",
    "            metaDescription = metaDescription.get('content')\n",
    "    \n",
    "    except:\n",
    "        #SET NaN values if the code excepts\n",
    "        metaTitle = \"NaN\"\n",
    "        metaDescription = \"NaN\"\n",
    "        \n",
    "    return metaTitle, metaDescription\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSiteDetails2(csv):\n",
    "    #Create an empty DF\n",
    "    d = pd.DataFrame()\n",
    "\n",
    "    for url in csv['Ur']:\n",
    "        hostname = urlparse(url).netloc\n",
    "\n",
    "        try:\n",
    "            wordsTotal = findContentLength(url)\n",
    "            numberImage = scanMediaInfo(url)[0]\n",
    "            Alt = scanMediaInfo(url)[1]\n",
    "\n",
    "            temp = pd.DataFrame({\n",
    "                'Domain': hostname,\n",
    "                'url': url,\n",
    "                'Total Images': numberImage,\n",
    "                'Image Tags': [Alt],\n",
    "                'Content-length': wordsTotal,\n",
    "            }, index=[0])\n",
    "\n",
    "            d = pd.concat([d, temp])\n",
    "        except:\n",
    "            numberImage = 0\n",
    "            Alt = \"NaN\"\n",
    "            temp = pd.DataFrame({\n",
    "                'Domain': hostname,\n",
    "                'url': url,\n",
    "                'Total Images': 0,\n",
    "                'Image Tags': \"NaN\",\n",
    "                'Content-length': wordsTotal,\n",
    "            }, index=[0])\n",
    "\n",
    "            d = pd.concat([d, temp])\n",
    "            continue\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllMeta(csv):\n",
    "    #empty df\n",
    "    emptyDf = pd.DataFrame()\n",
    "    #start for-loop\n",
    "    for url in csv['Ur']:\n",
    "        try:\n",
    "            #Get META-Title\n",
    "            titleMeta = getMetaDetails(url)[0]\n",
    "            #Get META-Description \n",
    "            metaDescriptionTag = getMetaDetails(url)[1]\n",
    "            #Create TEMPORARY DF\n",
    "            temp2 = pd.DataFrame({\n",
    "                'url': url,\n",
    "                'META-Title': titleMeta,\n",
    "                'META-Description': metaDescriptionTag\n",
    "            }, index=[0])\n",
    "\n",
    "            emptyDf = pd.concat([emptyDf, temp2])\n",
    "        except:\n",
    "            numberImage = 0\n",
    "            Alt = \"NaN\"\n",
    "            #Create TEMPORARY DF\n",
    "            temp2 = pd.DataFrame({\n",
    "                'url': url,\n",
    "                'META-Title': \"NaN\",\n",
    "                'META-Description': \"NaN\"\n",
    "            }, index=[0])\n",
    "\n",
    "            emptyDf = pd.concat([emptyDf, temp2])\n",
    "            continue\n",
    "    return emptyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDF(csv):\n",
    "    #Get return from siteDetails2()\n",
    "    dfList1 = getSiteDetails2(csv)\n",
    "    #Get return from META Function for TITLE & Description\n",
    "    dfList2 = getAllMeta(csv)\n",
    "    # Get return from Domain Age finder\n",
    "    dfList3 = getDomainAge(csv)\n",
    "    #Merge the first 2 functions returns\n",
    "    completeDf = pd.merge(dfList1,dfList2,left_on='url', right_on='url')\n",
    "    #Merge the last return into the complete DF\n",
    "    completeDf = pd.merge(completeDf,dfList3,left_on='url', right_on='url')\n",
    "    \n",
    "    #Complete DF Returned from all functions\n",
    "    return completeDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDomainAge(csv):\n",
    "    #Empty DF\n",
    "    emptyDf2 = pd.DataFrame()\n",
    "    \n",
    "    for url in csv['Ur']:\n",
    "        try:\n",
    "            #Must have to get WHOIS information\n",
    "            domain_name = url\n",
    "            whois_info = whois.whois(domain_name)\n",
    "            creationDate = whois_info.creation_date\n",
    "            #temporary DF\n",
    "            temp3 = pd.DataFrame({\n",
    "                'url': url,               \n",
    "                'Creation Date': creationDate\n",
    "            }, index=[0])\n",
    "\n",
    "            emptyDf2 = pd.concat([emptyDf2, temp3])\n",
    "        except:\n",
    "            #temporary DF\n",
    "            temp3 = pd.DataFrame({\n",
    "                'url': url,\n",
    "                'Creation Date': \"None\"\n",
    "            }, index=[0])\n",
    "\n",
    "            emptyDf2 = pd.concat([emptyDf2, temp3])\n",
    "            continue\n",
    "    \n",
    "    #Return the DF with Creation date of the domain.\n",
    "    return emptyDf2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
