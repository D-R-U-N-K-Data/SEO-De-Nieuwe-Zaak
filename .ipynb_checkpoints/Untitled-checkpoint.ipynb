{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/actie\n",
      "/bouwmaterialen\n",
      "/hout\n",
      "/gereedschap\n",
      "/sanitair\n",
      "/verwarming-en-klimaat\n",
      "/verf\n",
      "/elektra\n",
      "/verlichting\n",
      "/lijm-kit-en-pur\n",
      "/bevestigingsmateriaal\n",
      "/hang-en-sluitwerk\n",
      "/keukens\n",
      "/schoonmaak-auto-en-verhuizen\n",
      "/meer-minder\n",
      "/opop\n",
      "/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "def is_valid(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    urls = set()\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    \n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        \n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        \n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                print(f\"[!] External link: {href}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        print(f\"[*] Internal link: {href}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "        \n",
    "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "    print(\"[+] Total External links:\", len(external_urls))\n",
    "\n",
    "get_all_website_links('https://www.bouwmaat.nl/bouwmaterialen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unnecessary code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "# dikz = 0\n",
    "  \n",
    "# # function to extract html document from given url\n",
    "# def getHTMLdocument(url):\n",
    "      \n",
    "#     # request for HTML document of given url\n",
    "#     response = requests.get(url)\n",
    "      \n",
    "#     # response will be provided in JSON format\n",
    "#     return response.text\n",
    "  \n",
    "    \n",
    "# # assign required credentials\n",
    "# # assign URL\n",
    "# url_to_scrape = \"https://www.bouwmaterialenkopen.com/\"\n",
    "  \n",
    "# # create document\n",
    "# html_document = getHTMLdocument(url_to_scrape)\n",
    "  \n",
    "# # create soap object\n",
    "# soup = BeautifulSoup(html_document, 'html.parser')\n",
    "  \n",
    "  \n",
    "# # find all the anchor tags with \"href\" \n",
    "# # attribute starting with \"https://\"\n",
    "# for link in soup.find_all('a', attrs={'href': re.compile(\"^https://\")}):\n",
    "#     # display the actual urls\n",
    "#     print(link.get('href'))\n",
    "#     dikz = dikz + 1\n",
    "#     print(dikz)\n",
    "\n",
    "# from bs4 import BeautifulSoup \n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# from urllib.parse import urlparse\n",
    "# import re\n",
    "# import requests\n",
    "# from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# def findExternalLinks(siteUrl):\n",
    "\n",
    "#         req = requests.get(siteUrl)\n",
    "#         soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        \n",
    "#         links = [x.get('href') for x in soup.findAll('a')]\n",
    "        \n",
    "#         for a in links:\n",
    "#             a = str(a)\n",
    "            \n",
    "            \n",
    "#             if(a.startswith('http')):\n",
    "#                  print(a)\n",
    "                    \n",
    "                    \n",
    "#             elif(a.startswith('/')): \n",
    "                \n",
    "#                 if(siteUrl.endswith(\"/\")):\n",
    "#                     link = siteUrl[:-1]\n",
    "#                 else:\n",
    "#                     link = siteUrl\n",
    "                    \n",
    "#                 try:\n",
    "#                     code = urlopen(link + a).code\n",
    "#                     print(link + a + \"WWWWWWWWWWWWWWWWWWWW\")\n",
    "#                 except:\n",
    "#                     print(a + \"Cant OPEN! \" + link + a)\n",
    "                    \n",
    "                    \n",
    "#             elif(a.startswith('..')):\n",
    "                \n",
    "#                 if(siteUrl.endswith(\"/\")):\n",
    "#                     link = siteUrl[:-1]\n",
    "#                 else:\n",
    "#                     link = siteUrl\n",
    "                    \n",
    "#                 a = a[2:]\n",
    "#                 print(a)\n",
    "                \n",
    "#                 try:\n",
    "#                     code = urlopen(link + a).code\n",
    "#                     print(link + a + \"AAAAAAAAAAAAAAAAA\")\n",
    "#                 except:\n",
    "#                     print(\"Cant OPEN2!\")\n",
    "                    \n",
    "                    \n",
    "#             else:\n",
    "#                 print(a + \"i dont start good\")\n",
    "                \n",
    "# findExternalLinks('http://www.hout-en-bouwmaterialen.nl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "try:\n",
    "    code = urlopen(\"https://www.thuiswinkel.org/leden/bouwmaterialenkopen.com/certificaat\").code\n",
    "    print('omega=toaster')\n",
    "except: \n",
    "    print('toaster')\n",
    "    \n",
    "lol = \"https://www.bouwmaterialenkopen.com\"   \n",
    "domain = urlparse(lol).netloc\n",
    "print(domain) # --> www.example.test\n",
    "domain = \"https://\" + domain\n",
    "print(domain)\n",
    "\n",
    "try:\n",
    "    code = urlopen(\"https://www.bouwmaterialenkopen.com/klussen/zwembad-bouwen/\").code\n",
    "    print('omega=toaster')\n",
    "except: \n",
    "    print('toaster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = \"https://www.bouwmaterialenkopen.com/klussen/zwembad-bouwen/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    code = urlopen(ab).code\n",
    "    print(ab)\n",
    "except:\n",
    "    print('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lollolol(siteUrll):            \n",
    "    if(siteUrll.startswith('http')):\n",
    "        print(siteUrll + \"w\")\n",
    "    elif(siteUrll.startswith('/')): \n",
    "        domain = urlparse(siteUrll).netloc\n",
    "        domain = \"https://\" + domain\n",
    "                \n",
    "        try:\n",
    "            code = urlopen(\"https://bouwmaterialenkopen.com/\" + siteUrll).code\n",
    "            print(domain + siteUrll + \"WWWWWWWWWWWWWWWWWWWW\")\n",
    "        except:\n",
    "            print(siteUrll + \"Cant OPEN!\")\n",
    "    else:\n",
    "        print(siteUrll + \"i dont start good\")\n",
    "                \n",
    "lollolol('https://www.limtrade.nl/af-bouw/bouwmaterialen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
